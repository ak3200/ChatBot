import os
import json
import faiss
import re
import time
import numpy as np
import requests
from bs4 import BeautifulSoup
from fastapi import FastAPI
from pydantic import BaseModel
from PyPDF2 import PdfReader
from sentence_transformers import SentenceTransformer
from groq import Groq
import threading
from threading import Lock
import uvicorn

# --- Constants ---
CHUNKS_FILE = "chunks.jsonl"
PROCESSED_URLS_FILE = "processed_urls.txt"
PROCESSED_PDFS_FILE = "processed_pdfs.txt"
CHAT_HISTORY_FILE = "chat_history.jsonl"
PDF_DIR = "pdfs"
URLS_FILE = "urls.txt"
INDEX_FILE = "faiss_index.bin"
LLM_MODEL = "llama-3.3-70b-versatile"
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
REFRESH_INTERVAL = 3600  # 1 hour
FAILED_URLS_FILE = "failed_urls.txt"

# --- Init ---
model = SentenceTransformer("all-MiniLM-L6-v2")
embedding_dim = 384
index = faiss.IndexFlatL2(embedding_dim)
chunk_texts = []
chunk_sources = []
groq_client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None
data_lock = Lock()

app = FastAPI()

# --- Utilities ---
def sanitize_string(s):
    # Remove control characters except newline, carriage return, tab
    return re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', s)

def clean_response(response):
    # Replace all newlines, tabs, and |n (if it exists) with spaces
    response = response.replace("\n", " ").replace("\t", " ").replace("|n", " ")
    # Collapse multiple spaces into one
    response = re.sub(r'\s+', ' ', response).strip()
    return response

def chunk_text(text, chunk_size=500, overlap=50):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks

def save_chunks(chunks, source):
    with open(CHUNKS_FILE, 'a', encoding='utf-8') as f:
        for chunk in chunks:
            f.write(json.dumps({"chunk": chunk, "source": source}) + "\n")

def embed_and_index_chunks(chunks, source):
    global chunk_texts, chunk_sources
    embeddings = model.encode(chunks)
    with data_lock:
        index.add(np.array(embeddings))
        chunk_texts.extend(chunks)
        chunk_sources.extend([source] * len(chunks))
    save_chunks(chunks, source)
    save_faiss_index()

def fetch_clean_text_from_url(url, max_retries=3, backoff_factor=1):
    for attempt in range(1, max_retries + 1):
        try:
            res = requests.get(url, timeout=30)
            if res.status_code == 200:
                soup = BeautifulSoup(res.content, "html.parser")
                text = soup.get_text(separator=" ", strip=True)
                return re.sub(r'\s+', ' ', text)
            else:
                print(f"[!] Non-200 status code {res.status_code} for {url} (attempt {attempt})")
        except Exception as e:
            print(f"[!] Attempt {attempt} failed for {url}: {e}")
        if attempt < max_retries:
            time.sleep(backoff_factor * (2 ** (attempt - 1)))
    with open(FAILED_URLS_FILE, "a") as f:
        f.write(url + "\n")
    return ""

def extract_links_from_page(url):
    try:
        res = requests.get(url, timeout=30)
        soup = BeautifulSoup(res.content, "html.parser")
        return [tag['href'] for tag in soup.find_all("a", href=True) if tag['href'].startswith("http")]
    except Exception as e:
        print(f"[!] Failed to extract links from {url}: {e}")
        with open(FAILED_URLS_FILE, "a") as f:
            f.write(url + "\n")
        return []

def load_processed(file_path):
    return set(open(file_path).read().splitlines()) if os.path.exists(file_path) else set()

def append_processed(file_path, item):
    with open(file_path, "a") as f:
        f.write(item + "\n")

def save_faiss_index():
    faiss.write_index(index, INDEX_FILE)
    print(f"Saved FAISS index with {index.ntotal if hasattr(index, 'ntotal') else 'N/A'} embeddings")

def load_faiss_index():
    if os.path.exists(INDEX_FILE):
        try:
            loaded_index = faiss.read_index(INDEX_FILE)
            print(f"Loaded FAISS index with {loaded_index.ntotal if hasattr(loaded_index, 'ntotal') else 'N/A'} embeddings")
            return loaded_index
        except Exception as e:
            print(f"[!] Failed to load FAISS index: {e}")
            return None
    return None

def background_refresh():
    while True:
        time.sleep(REFRESH_INTERVAL)
        print("[*] Background refresh started")
        process_urls()
        process_pdfs()
        save_faiss_index()

def check_duplicates(file_path):
    with open(file_path, 'r') as f:
        lines = [line.strip() for line in f if line.strip()]
    unique = set(lines)
    print(f"Total lines: {len(lines)}")
    print(f"Unique lines: {len(unique)}")
    duplicates = [url for url in lines if lines.count(url) > 1]
    duplicates = list(set(duplicates))
    print(f"Duplicate URLs: {duplicates}")

def improve_query(session_id, question):
    history = []
    if os.path.exists(CHAT_HISTORY_FILE):
        with open(CHAT_HISTORY_FILE, 'r') as f:
            for line in f:
                entry = json.loads(line)
                if entry['session_id'] == session_id:
                    history.append(entry['question'])
    if history:
        context = " Context: " + " | ".join(history[-3:])
        return question + context
    return question

def is_complex_question(question):
    return ("difference between" in question.lower() or
            "compare" in question.lower() or
            "versus" in question.lower() or
            "vs" in question.lower() or
            "explain how" in question.lower())

def is_comparative_model_difference_question(question):
    keywords = ["difference between", "compare", "versus", "vs", "difference of", "compare with"]
    question_lower = question.lower()
    return any(keyword in question_lower for keyword in keywords)

def extract_device_and_models(question):
    # Improved regex to match "model1 and model2", "model1 vs model2", "model1-model2", etc.
    pattern = r"(?:(?:[A-Za-z0-9]+-)?[A-Za-z0-9]+)\s*(?:and|vs|versus|,|-)\s*((?:[A-Za-z0-9]+-)?[A-Za-z0-9]+)"
    # Find all matches
    matches = re.findall(r"([A-Za-z0-9]+-[A-Za-z0-9]+|[A-Za-z0-9]+)", question, re.IGNORECASE)
    if len(matches) >= 2:
        model1 = matches[0]
        model2 = matches[1]
        # Try to extract device (prefix before hyphen, if any)
        if '-' in model1 and '-' in model2:
            device1, model1_suffix = model1.split('-', 1)
            device2, model2_suffix = model2.split('-', 1)
            if device1 == device2:
                return device1, model1_suffix, model2_suffix
            else:
                return None, model1, model2
        else:
            return None, model1, model2
    return None, None, None

def get_model_chunks(device, model):
    model_chunks = []
    search_term = f"{device}-{model}" if device else model
    for i, (chunk, source) in enumerate(zip(chunk_texts, chunk_sources)):
        if search_term.lower() in chunk.lower():
            model_chunks.append({"chunk": chunk, "source": source})
    return model_chunks

def generate_comparison_prompt(device, model1, model2, model1_chunks, model2_chunks):
    model1_full = f"{device}-{model1}" if device else model1
    model2_full = f"{device}-{model2}" if device else model2
    model1_context = "\n\n".join([f"Source: {c['source']}\nContent: {c['chunk']}" for c in model1_chunks])
    model2_context = "\n\n".join([f"Source: {c['source']}\nContent: {c['chunk']}" for c in model2_chunks])
    prompt = f"""You are comparing two models: {model1_full} and {model2_full}. Here is information about each model:

Model 1: {model1_full}
{model1_context}

Model 2: {model2_full}
{model2_context}

Analyze the differences between these two models. Highlight key differences in features, specifications, or capabilities.
Present the answer in a neat, easy-to-read format. Use clear section headers and bullet points if possible.
Do not use extra line breaks or tabs.

Differences:
"""
    return prompt

def generate_llm_response(question, context_chunks=None, use_own_knowledge=False):
    if use_own_knowledge:
        prompt = f"""Letâ€™s think step by step. Answer the following question using your own knowledge and reasoning.
If the question is about a comparison or difference, provide a clear and concise answer.
Present the answer in a neat, easy-to-read format. Use clear section headers and bullet points if possible.
Do not use extra line breaks or tabs.
If you are unsure, say so.

Question: {question}

Answer:
"""
    else:
        context = "\n\n".join([f"Source: {chunk['source']}\nContent: {chunk['chunk']}" for chunk in context_chunks])
        prompt = f"""Answer the user's question based only on the provided context.
If the answer is not present in the context, say "I don't know" or "The information is not available."
Do not make up answers or guess. Be accurate and concise.
Present the answer in a neat, easy-to-read format. Use clear section headers and bullet points if possible.
Do not use extra line breaks or tabs.
Do not mention the source of the information or use phrases like "according to the provided context".
Simply state the answer as a fact.

Context:
{context}

Question: {question}

Answer:
"""
    try:
        response = groq_client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model=LLM_MODEL,
            temperature=0.3,
            max_tokens=512
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"LLM generation error: {e}")
        return "I couldn't generate a response at this time."

def save_chat_history(session_id, question, response, feedback=None):
    with open(CHAT_HISTORY_FILE, "a", encoding='utf-8') as f:
        f.write(json.dumps({
            "session_id": session_id,
            "question": question,
            "response": response,
            "feedback": feedback,
            "timestamp": time.time()
        }) + "\n")

def load_chunks():
    global chunk_texts, chunk_sources
    chunk_texts.clear()
    chunk_sources.clear()
    if os.path.exists(CHUNKS_FILE):
        with open(CHUNKS_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                chunk_texts.append(data['chunk'])
                chunk_sources.append(data['source'])
        print(f"[*] Loaded {len(chunk_texts)} chunks from {CHUNKS_FILE}")

def reindex_all_chunks():
    global index, chunk_texts, chunk_sources
    if not chunk_texts:
        return
    index = faiss.IndexFlatL2(embedding_dim)
    for i in range(0, len(chunk_texts), 1000):
        batch = chunk_texts[i:i+1000]
        embeddings = model.encode(batch)
        index.add(np.array(embeddings))
    print(f"Reindexed {len(chunk_texts)} chunks into FAISS index")
    save_faiss_index()

def process_urls():
    processed = load_processed(PROCESSED_URLS_FILE)
    failed = load_processed(FAILED_URLS_FILE)
    if not os.path.exists(URLS_FILE):
        print("[!] urls.txt not found")
        return
    with open(URLS_FILE) as f:
        urls = [line.strip() for line in f if line.strip()]
    print(f"[*] Total URLs in urls.txt: {len(urls)}")
    print(f"[*] Already processed: {len(processed)}")
    print(f"[*] Already failed: {len(failed)}")
    processed_count = 0
    for url in urls:
        if url in processed or url in failed:
            continue
        print(f"[+] Processing URL: {url}")
        all_links = set([url]) | set(extract_links_from_page(url))
        for link in all_links:
            if link in processed or link in failed:
                continue
            text = fetch_clean_text_from_url(link)
            print(f"Extracted text length: {len(text)}")
            if text:
                chunks = chunk_text(text)
                print(f"Generated chunks: {len(chunks)}")
                embed_and_index_chunks(chunks, link)
                append_processed(PROCESSED_URLS_FILE, link)
                processed_count += 1
    print(f"[*] Processed {processed_count} new URLs and their links")
    check_duplicates("processed_urls.txt")

def process_pdfs():
    processed = load_processed(PROCESSED_PDFS_FILE)
    if not os.path.exists(PDF_DIR):
        print("[!] PDF directory not found.")
        return
    for fname in os.listdir(PDF_DIR):
        if not fname.endswith(".pdf") or fname in processed:
            continue
        path = os.path.join(PDF_DIR, fname)
        try:
            reader = PdfReader(path)
            text = " ".join(page.extract_text() or "" for page in reader.pages)
            text = re.sub(r'\s+', ' ', text)
            print(f"[+] Processing PDF: {fname}")
            print(f"Extracted text length: {len(text)}")
            chunks = chunk_text(text)
            print(f"Generated chunks: {len(chunks)}")
            embed_and_index_chunks(chunks, fname)
            append_processed(PROCESSED_PDFS_FILE, fname)
        except Exception as e:
            print(f"[!] Failed to process {fname}: {e}")

class QueryPayload(BaseModel):
    question: str
    session_id: str = "default"

class FeedbackPayload(BaseModel):
    session_id: str
    question: str
    response: str
    feedback: str  # e.g., "correct", "incorrect", "unclear"

@app.post("/feedback/")
async def submit_feedback(payload: FeedbackPayload):
    save_chat_history(payload.session_id, payload.question, payload.response, payload.feedback)
    return {"status": "success"}

@app.post("/ask/")
async def ask_question(payload: QueryPayload):
    question = sanitize_string(payload.question)
    session_id = payload.session_id or "default"
    improved_question = improve_query(session_id, question)

    # Handle model comparison queries
    if is_comparative_model_difference_question(improved_question):
        device, model1, model2 = extract_device_and_models(improved_question)
        if model1 and model2:
            model1_chunks = get_model_chunks(device, model1)
            model2_chunks = get_model_chunks(device, model2)

            if not model1_chunks or not model2_chunks:
                response = "I couldn't find enough information for a detailed comparison between those models."
            else:
                prompt = generate_comparison_prompt(device, model1, model2, model1_chunks, model2_chunks)
                response = generate_llm_response(prompt, use_own_knowledge=False)
        else:
            response = "Please provide two valid model names to compare."

        response = clean_response(response)
        save_chat_history(session_id, question, response)
        return {"response": response}

    # Regular question answering with vector search
    question_embedding = model.encode([improved_question])
    with data_lock:
        D, I = index.search(np.array(question_embedding), k=5)
        context_chunks = [{"chunk": chunk_texts[i], "source": chunk_sources[i]} for i in I[0] if i < len(chunk_texts)]

    # Detect ambiguity based on multiple matching device models
    device_mentions = set()
    for chunk in context_chunks:
        matches = re.findall(r"\b([A-Z]+-\w+)\b", chunk["chunk"])
        device_mentions.update(matches)

    if len(device_mentions) > 1:
        response = f"Your question mentions a device type that has multiple models: {sorted(device_mentions)}. Please specify the exact model for a precise answer."
    else:
        response = generate_llm_response(improved_question, context_chunks=context_chunks, use_own_knowledge=False)

    response = clean_response(response)
    save_chat_history(session_id, question, response)
    return {"response": response}


@app.on_event("startup")
def startup_event():
    global index
    loaded_index = load_faiss_index()
    if loaded_index:
        index = loaded_index
        print("[*] Loaded existing FAISS index")
    else:
        print("[*] Initializing new FAISS index")
    load_chunks()
    if index.ntotal == 0:
        reindex_all_chunks()
    print("[*] Loading content from URLs and PDFs...")
    process_urls()
    process_pdfs()
    print("[*] Data loaded.")
    print(f"Chunk texts after load: {len(chunk_texts)}")
    print(f"Chunk sources after load: {len(chunk_sources)}")
    print(f"Index size after load: {index.ntotal if hasattr(index, 'ntotal') else 'N/A'}")
    refresh_thread = threading.Thread(target=background_refresh, daemon=True)
    refresh_thread.start()

if __name__ == "__main__":
    uvicorn.run(app, host="127.0.0.1", port=8000)

import os
import time
import json
import asyncio
import httpx
import requests
import re
import difflib
import uvicorn
from typing import List
from fastapi import FastAPI, HTTPException, Body
from PyPDF2 import PdfReader
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from dotenv import load_dotenv
load_dotenv()

# Constants
SESSION_ID = "default"
CHAT_DIR = "chats"
CHUNKS_FILE = "chunks.jsonl"
PROCESSED_PDFS_FILE = "processed_pdfs.txt"
PROCESSED_URLS_FILE = "processed_urls.txt"
LAST_AMBIGUOUS_QUESTION_FILE = "last_ambiguous_question.json"
URLS_FILE = "urls.txt"
PDF_FOLDER = "pdfs"

# Globals
model = SentenceTransformer("all-MiniLM-L6-v2")
os.makedirs(CHAT_DIR, exist_ok=True)

# Utility paths
def chat_path(session_id=SESSION_ID):
    return os.path.join(CHAT_DIR, f"{session_id}.json")

def load_processed(path):
    return set(open(path).read().splitlines()) if os.path.exists(path) else set()

def save_processed(path, entries):
    with open(path, "w") as f:
        f.write("\n".join(entries))

def append_chunks(chunks):
    with open(CHUNKS_FILE, "a") as f:
        for entry in chunks:
            f.write(json.dumps(entry) + "\n")

def load_all_chunks():
    if not os.path.exists(CHUNKS_FILE):
        return []
    with open(CHUNKS_FILE) as f:
        return [json.loads(line) for line in f]

def extract_text_from_pdf(filepath):
    reader = PdfReader(filepath)
    return "\n".join(page.extract_text() or "" for page in reader.pages)

def chunk_text(text, chunk_size=500, overlap=50):
    chunks, start = [], 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks

def embed_and_store_chunks(chunks, source):
    embeddings = model.encode(chunks)
    entries = [{"chunk": chunk, "embedding": emb.tolist(), "source": source} for chunk, emb in zip(chunks, embeddings)]
    append_chunks(entries)

def vector_search(query, all_chunks, top_k=5):
    if not all_chunks:
        return []
    q_embed = model.encode([query])
    embeddings = [c["embedding"] for c in all_chunks]
    similarities = cosine_similarity(q_embed, embeddings)[0]
    top_indices = similarities.argsort()[-top_k:][::-1]
    return [all_chunks[i] for i in top_indices]

async def fetch_url_text(url, timeout=60, retries=3):
    for attempt in range(retries):
        try:
            async with httpx.AsyncClient(timeout=timeout) as client:
                response = await client.get(url)
                if "captcha" in response.text.lower():
                    raise ValueError("Blocked by CAPTCHA")
                return response.text
        except Exception:
            await asyncio.sleep(2 ** attempt)
    return None

async def process_all_urls():
    urls = open(URLS_FILE).read().splitlines() if os.path.exists(URLS_FILE) else []
    processed = load_processed(PROCESSED_URLS_FILE)
    new_urls = [url for url in urls if url not in processed]
    print(f"[INFO] Processing {len(new_urls)} new URLs...")

    tasks = [fetch_url_text(url) for url in new_urls]
    results = await asyncio.gather(*tasks)

    for url, text in zip(new_urls, results):
        if text:
            chunks = chunk_text(text)
            embed_and_store_chunks(chunks, source=url)
            processed.add(url)

    save_processed(PROCESSED_URLS_FILE, processed)

def process_all_pdfs():
    pdfs = [f for f in os.listdir(PDF_FOLDER) if f.endswith(".pdf")]
    processed = load_processed(PROCESSED_PDFS_FILE)
    new_pdfs = [f for f in pdfs if f not in processed]
    print(f"[INFO] Processing {len(new_pdfs)} new PDFs...")

    for f in new_pdfs:
        path = os.path.join(PDF_FOLDER, f)
        text = extract_text_from_pdf(path)
        chunks = chunk_text(text)
        embed_and_store_chunks(chunks, source=f)
        processed.add(f)

    save_processed(PROCESSED_PDFS_FILE, processed)

def save_chat(session_id, role, message):
    path = chat_path(session_id)
    chat = json.load(open(path)) if os.path.exists(path) else []
    chat.append({"role": role, "message": message})
    with open(path, "w") as f:
        json.dump(chat, f)

def extract_devices_and_models(all_chunks):
    device_models = {}

    # Regex to find device-model patterns like 'RS485-CB', 'XYZ-123', etc.
    pattern = re.compile(r"\b([A-Z]{2,}[A-Z0-9]*)-([A-Z0-9]{2,})\b")

    for chunk in all_chunks:
        matches = pattern.findall(chunk["chunk"])
        for device, model_ in matches:
            # Filter out models that are digits-only or device too short
            if model_.isdigit() or len(device) <= 1:
                continue
            device_models.setdefault(device, set()).add(model_)

    # Only keep devices with multiple distinct models (ambiguity possible)
    return {k: sorted(v) for k, v in device_models.items() if len(v) > 1}

def model_mentioned(user_query, known_models):
    query = user_query.upper()
    for model in known_models:
        if model and (model in query or query.endswith(f"-{model}") or query.endswith(model)):
            return model
    # Try fuzzy match for near matches
    close = difflib.get_close_matches(query, known_models, n=1, cutoff=0.8)
    return close[0] if close else None

def load_model_memory(session_id):
    path = f"{session_id}_model_memory.json"
    return json.load(open(path)) if os.path.exists(path) else {}

def save_model_memory(session_id, memory):
    path = f"{session_id}_model_memory.json"
    with open(path, "w") as f:
        json.dump(memory, f)

def call_groq(prompt):
    try:
        response = requests.post(
            "https://api.groq.com/openai/v1/chat/completions",
            headers={"Authorization": f"Bearer {os.environ['GROQ_API_KEY']}"},
            json={
                "model": "llama3-70b-8192",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.2,
                "max_tokens": 200
            },
            timeout=60
        )
        result = response.json()
        return result["choices"][0]["message"]["content"].strip()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"GROQ call failed: {e}")

app = FastAPI()

@app.on_event("startup")
async def preload_data():
    process_all_pdfs()
    await process_all_urls()

@app.get("/status")
def status():
    chunks = load_all_chunks()
    return {
        "chunks_loaded": len(chunks),
        "source_count": len(set(c["source"] for c in chunks))
    }

@app.post("/chat")
def chat(user_query: str = Body(...), session_id: str = Body(SESSION_ID)):
    save_chat(session_id, "user", user_query)
    chunks = load_all_chunks()
    device_models = extract_devices_and_models(chunks)
    model_memory = load_model_memory(session_id)

    # Check if user's query refers to any device with multiple models
    ambiguous_device = None
    selected_model = None
    user_query_upper = user_query.upper()

    for device, models in device_models.items():
        if device in user_query_upper:
            # Check if user query specifies a known model
            selected_model = model_mentioned(user_query_upper, models)
            if selected_model:
                # Save the model choice for this device
                model_memory[device] = selected_model
                save_model_memory(session_id, model_memory)
            else:
                # No model mentioned, check if previously remembered
                if device in model_memory:
                    selected_model = model_memory[device]
                else:
                    # Need clarification: multiple models found, none specified
                    ambiguous_device = device
                    break

    # If ambiguous device found and no model selected yet
    if ambiguous_device and not selected_model:
        # Save last ambiguous question for follow-up
        with open(LAST_AMBIGUOUS_QUESTION_FILE, "w") as f:
            json.dump({"session_id": session_id, "question": user_query, "device": ambiguous_device}, f)
        return {
            "response": f"Multiple models found for device '{ambiguous_device}': {', '.join(device_models[ambiguous_device])}. Please specify the model."
        }

    # Handle follow-up clarification if any
    if os.path.exists(LAST_AMBIGUOUS_QUESTION_FILE):
        last = json.load(open(LAST_AMBIGUOUS_QUESTION_FILE))
        if last.get("session_id") == session_id:
            device = last.get("device")
            # Check if user specified model now in query
            clarified_model = None
            if device and device in device_models:
                clarified_model = model_mentioned(user_query_upper, device_models[device])
            if clarified_model:
                # Save clarified model and remove last ambiguous question
                model_memory[device] = clarified_model
                save_model_memory(session_id, model_memory)
                # Rephrase query with model
                user_query = f"{last['question']} (Model specified: {clarified_model})"
                os.remove(LAST_AMBIGUOUS_QUESTION_FILE)

    # Now search with possibly clarified query
    relevant = vector_search(user_query, chunks)
    context = "\n".join(f"[{r['source']}]: {r['chunk']}" for r in relevant)
    final_prompt = (
        "You are a technical assistant. Only use the below context to answer. Be brief. "
        "Do NOT ask users to refer to datasheets or manuals.\n\n"
        f"{context}\n\nQuestion: {user_query}\nAnswer:"
    )
    answer = call_groq(final_prompt)
    save_chat(session_id, "assistant", answer)
    return {"response": answer}

# ---- Run Locally ----
if __name__ == "__main__":
    uvicorn.run(app, host="127.0.0.1", port=8000)
